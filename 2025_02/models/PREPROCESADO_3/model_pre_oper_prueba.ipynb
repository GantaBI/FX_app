{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio (MSE): 36.05610909090909\n",
      "Error absoluto medio (MAE): 3.0963636363636367\n",
      "Error absoluto mediano: 1.8050000000000004\n",
      "R² (coeficiente de determinación): 0.16815078114758986\n",
      "Varianza explicada: 0.16816681642137876\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Cargamos los datos desde el CSV\n",
    "df = pd.read_csv('DATOS_PREPROCESADOS_3.csv')\n",
    "\n",
    "# Eliminamos la columna identificadora que no aporta a la predicción\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Convertimos la variable categórica 'gdiagalt' a variables dummy (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"gdiagalt\"], drop_first=True)\n",
    "\n",
    "# Definimos la variable objetivo y las características.\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creamos y entrenamos el modelo Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizamos predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculamos las métricas\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "# Imprimimos las métricas\n",
    "print(\"Error cuadrático medio (MSE):\", mse)\n",
    "print(\"Error absoluto medio (MAE):\", mae)\n",
    "print(\"Error absoluto mediano:\", medae)\n",
    "print(\"R² (coeficiente de determinación):\", r2)\n",
    "print(\"Varianza explicada:\", explained_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento este es el mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__n_estimators': 100}\n",
      "MSE: 31.2457605770309\n",
      "MAE: 2.7462128183000734\n",
      "R²: 0.2791301617454349\n",
      "Varianza explicada: 0.2821245144497967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.read_csv('/home/ubuntu/STG-fractura_cadera/2025_02/models/PREPROCESADO_3/DATOS_PREPROCESADOS_3.csv')\n",
    "\n",
    "# Eliminar filas con \"-999\" o -999 en cualquier columna\n",
    "df = df[~df.isin([\"-999\", -999]).any(axis=1)]\n",
    "\n",
    "# Elimina la columna identificadora\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Seleccionamos X e y. Eliminamos de X las columnas que no queremos usar como features\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Identificamos columnas numéricas y categóricas (suponiendo que las de tipo object son categóricas)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Creamos un preprocesador que escale los numéricos y aplique OneHotEncoder a los categóricos\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Definimos un Pipeline que primero preprocesa y luego entrena un modelo de Gradient Boosting\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuramos un GridSearch para ajustar hiperparámetros del Gradient Boosting\n",
    "param_grid = {\n",
    "    \"regressor__n_estimators\": [100, 200],\n",
    "    \"regressor__max_depth\": [3, 5, 7],\n",
    "    \"regressor__learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "# Realizamos predicciones sobre el conjunto de prueba\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculamos las métricas de rendimiento\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R²:\", r2)\n",
    "print(\"Varianza explicada:\", explained_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\nfrom xgboost import XGBRegressor\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\\n\\n# Cargamos el dataset\\ndf = pd.read_csv(\\'/home/ubuntu/STG-fractura_cadera/2025_02/models/PREPROCESADO_3/DATOS_PREPROCESADOS_3.csv\\')\\n\\n# Eliminar filas que contengan \"-999\" o -999 en cualquier columna\\ndf = df[~df.isin([\"-999\", -999]).any(axis=1)]\\n\\n# Eliminamos la columna identificadora\\ndf.drop(\"gidenpac\", axis=1, inplace=True)\\n\\n# Lista de variables categóricas\\ncat_features = [\\n    \\'gsitalta\\',\\'itipsexo\\', \\'itipingr\\', \\'ireingre\\', \\'iotrocen\\', \\'gdiagalt\\',\\n    \\'ds_izq_der\\', \\'ds_turno\\', \\'ds_dia_semana_llegada\\', \\'ds_mes_llegada\\',\\n    \\'ds_centro_afueras\\', \\'ds_alergia_medicamentosa\\',\\'movilidad\\', \\'riesgo_caida\\', \\'ds_alergia_alimenticia\\',\\n    \\'ds_otras_alergias\\', \\'ds_ITU\\', \\'ds_anemia\\',\\n    \\'ds_vitamina_d\\', \\'ds_insuficiencia_respiratoria\\', \\'ds_insuficiencia_cardiaca\\',\\n    \\'ds_deterioro_cognitivo\\', \\'ds_insuficiencia_renal\\', \\'ds_HTA\\', \\'ds_diabetes\\'\\n]\\n\\n# Convertir las variables de la lista a tipo \\'category\\'\\nfor col in cat_features:\\n    if col in df.columns:\\n        df[col] = df[col].astype(\"category\")\\n\\n# Verifica los tipos de datos\\nprint(\"Tipos de datos después de la conversión:\")\\nprint(df.dtypes)\\n\\n# Definimos las features y la variable objetivo\\n# En este ejemplo, eliminamos las columnas que no queremos usar como predictores\\nX = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\\ny = df[\"ds_pre_oper\"]\\n\\n# Identificamos columnas numéricas y categóricas:\\n# Aseguramos que las variables categóricas estén en la lista de columnas tipo \\'object\\' o \\'category\\'\\nnum_cols = X.select_dtypes(include=[np.number]).columns\\ncat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\\nprint(\"\\nVariables categóricas que se utilizarán:\", list(cat_cols))\\n\\n# Creamos un preprocesador: escalamos variables numéricas y codificamos variables categóricas\\npreprocessor = ColumnTransformer(transformers=[\\n    (\"num\", StandardScaler(), num_cols),\\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\\n])\\n\\n# Definimos un pipeline que incluya el preprocesamiento y el modelo XGBoost\\npipeline = Pipeline(steps=[\\n    (\"preprocessor\", preprocessor),\\n    (\"regressor\", XGBRegressor(random_state=42, objective=\"reg:squarederror\"))\\n])\\n\\n# Dividimos los datos en conjuntos de entrenamiento y prueba\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Definimos una búsqueda de hiperparámetros para XGBoost\\nparam_grid = {\\n    \"regressor__n_estimators\": [100, 200, 300],\\n    \"regressor__max_depth\": [3, 5, 7],\\n    \"regressor__learning_rate\": [0.01, 0.1, 0.2],\\n    \"regressor__subsample\": [0.8, 1.0]\\n}\\n\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\\ngrid_search.fit(X_train, y_train)\\n\\nprint(\"\\nMejores parámetros encontrados:\", grid_search.best_params_)\\n\\n# Predicciones y evaluación\\ny_pred = grid_search.predict(X_test)\\n\\nmse = mean_squared_error(y_test, y_pred)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nexplained_var = explained_variance_score(y_test, y_pred)\\n\\nprint(\"\\nError cuadrático medio (MSE):\", mse)\\nprint(\"Error absoluto medio (MAE):\", mae)\\nprint(\"R² (coeficiente de determinación):\", r2)\\nprint(\"Varianza explicada:\", explained_var)\\n\\n# Verificar qué columnas (features) se han generado en el preprocesamiento\\npreprocessor_final = grid_search.best_estimator_.named_steps[\"preprocessor\"]\\n\\n# Obtener los nombres de las columnas resultantes (requiere scikit-learn >= 1.0)\\ntry:\\n    feature_names = preprocessor_final.get_feature_names_out()\\n    print(\"\\nCaracterísticas utilizadas por el modelo:\")\\n    print(feature_names)\\nexcept AttributeError:\\n    print(\"\\nLa versión de scikit-learn no soporta \\'get_feature_names_out()\\'.\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Cargamos el dataset\n",
    "df = pd.read_csv('/home/ubuntu/STG-fractura_cadera/2025_02/models/PREPROCESADO_3/DATOS_PREPROCESADOS_3.csv')\n",
    "\n",
    "# Eliminar filas que contengan \"-999\" o -999 en cualquier columna\n",
    "df = df[~df.isin([\"-999\", -999]).any(axis=1)]\n",
    "\n",
    "# Eliminamos la columna identificadora\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Lista de variables categóricas\n",
    "cat_features = [\n",
    "    'gsitalta','itipsexo', 'itipingr', 'ireingre', 'iotrocen', 'gdiagalt',\n",
    "    'ds_izq_der', 'ds_turno', 'ds_dia_semana_llegada', 'ds_mes_llegada',\n",
    "    'ds_centro_afueras', 'ds_alergia_medicamentosa','movilidad', 'riesgo_caida', 'ds_alergia_alimenticia',\n",
    "    'ds_otras_alergias', 'ds_ITU', 'ds_anemia',\n",
    "    'ds_vitamina_d', 'ds_insuficiencia_respiratoria', 'ds_insuficiencia_cardiaca',\n",
    "    'ds_deterioro_cognitivo', 'ds_insuficiencia_renal', 'ds_HTA', 'ds_diabetes'\n",
    "]\n",
    "\n",
    "# Convertir las variables de la lista a tipo 'category'\n",
    "for col in cat_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Verifica los tipos de datos\n",
    "print(\"Tipos de datos después de la conversión:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Definimos las features y la variable objetivo\n",
    "# En este ejemplo, eliminamos las columnas que no queremos usar como predictores\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Identificamos columnas numéricas y categóricas:\n",
    "# Aseguramos que las variables categóricas estén en la lista de columnas tipo 'object' o 'category'\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "print(\"\\nVariables categóricas que se utilizarán:\", list(cat_cols))\n",
    "\n",
    "# Creamos un preprocesador: escalamos variables numéricas y codificamos variables categóricas\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Definimos un pipeline que incluya el preprocesamiento y el modelo XGBoost\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", XGBRegressor(random_state=42, objective=\"reg:squarederror\"))\n",
    "])\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definimos una búsqueda de hiperparámetros para XGBoost\n",
    "param_grid = {\n",
    "    \"regressor__n_estimators\": [100, 200, 300],\n",
    "    \"regressor__max_depth\": [3, 5, 7],\n",
    "    \"regressor__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"regressor__subsample\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMejores parámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "# Predicciones y evaluación\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nError cuadrático medio (MSE):\", mse)\n",
    "print(\"Error absoluto medio (MAE):\", mae)\n",
    "print(\"R² (coeficiente de determinación):\", r2)\n",
    "print(\"Varianza explicada:\", explained_var)\n",
    "\n",
    "# Verificar qué columnas (features) se han generado en el preprocesamiento\n",
    "preprocessor_final = grid_search.best_estimator_.named_steps[\"preprocessor\"]\n",
    "\n",
    "# Obtener los nombres de las columnas resultantes (requiere scikit-learn >= 1.0)\n",
    "try:\n",
    "    feature_names = preprocessor_final.get_feature_names_out()\n",
    "    print(\"\\nCaracterísticas utilizadas por el modelo:\")\n",
    "    print(feature_names)\n",
    "except AttributeError:\n",
    "    print(\"\\nLa versión de scikit-learn no soporta 'get_feature_names_out()'.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo: GradientBoosting\n",
      "Mejores parámetros: {'regressor__learning_rate': 0.01, 'regressor__max_depth': 3, 'regressor__n_estimators': 100}\n",
      "MSE: 31.2457605770309\n",
      "MAE: 2.7462128183000734\n",
      "R²: 0.2791301617454349\n",
      "Varianza explicada: 0.2821245144497967\n",
      "----------------------------------------\n",
      "Entrenando modelo: RandomForest\n",
      "Mejores parámetros: {'regressor__max_depth': None, 'regressor__n_estimators': 100}\n",
      "MSE: 36.51889090909091\n",
      "MAE: 3.162272727272727\n",
      "R²: 0.15747395817195964\n",
      "Varianza explicada: 0.15757556575105747\n",
      "----------------------------------------\n",
      "Entrenando modelo: LinearRegression\n",
      "Mejores parámetros: N/A\n",
      "MSE: 36.84171308712526\n",
      "MAE: 3.0865922407670454\n",
      "R²: 0.1500261391089256\n",
      "Varianza explicada: 0.15325534173958266\n",
      "----------------------------------------\n",
      "Entrenando modelo: SVR\n",
      "Mejores parámetros: {'regressor__C': 0.1, 'regressor__epsilon': 0.01}\n",
      "MSE: 44.98132363723606\n",
      "MAE: 2.7590994906316264\n",
      "R²: -0.03776252829278448\n",
      "Varianza explicada: 0.000980885689281119\n",
      "----------------------------------------\n",
      "Resumen comparativo:\n",
      "                                                        Best Params  \\\n",
      "GradientBoosting  {'regressor__learning_rate': 0.01, 'regressor_...   \n",
      "RandomForest      {'regressor__max_depth': None, 'regressor__n_e...   \n",
      "LinearRegression                                                N/A   \n",
      "SVR               {'regressor__C': 0.1, 'regressor__epsilon': 0.01}   \n",
      "\n",
      "                        MSE       MAE        R2 Explained Variance  \n",
      "GradientBoosting  31.245761  2.746213   0.27913           0.282125  \n",
      "RandomForest      36.518891  3.162273  0.157474           0.157576  \n",
      "LinearRegression  36.841713  3.086592  0.150026           0.153255  \n",
      "SVR               44.981324  2.759099 -0.037763           0.000981  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "# Importamos modelos adicionales\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.read_csv('/home/ubuntu/STG-fractura_cadera/2025_02/models/PREPROCESADO_3/DATOS_PREPROCESADOS_3.csv')\n",
    "\n",
    "# Eliminar filas con \"-999\" o -999 en cualquier columna\n",
    "df = df[~df.isin([\"-999\", -999]).any(axis=1)]\n",
    "\n",
    "# Eliminamos la columna identificadora\n",
    "df.drop(\"gidenpac\", axis=1, inplace=True)\n",
    "\n",
    "# Seleccionamos X e y. Eliminamos de X las columnas que no queremos usar como features\n",
    "X = df.drop([\"gsitalta\", \"ds_vivo_alta\", \"ds_estancia\", \"ds_post_oper\", \"ds_pre_oper\"], axis=1)\n",
    "y = df[\"ds_pre_oper\"]\n",
    "\n",
    "# Identificamos columnas numéricas y categóricas (suponiendo que las de tipo object son categóricas)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Preprocesador: escalamos numéricos y aplicamos OneHotEncoder a las categóricas\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Configuración de modelos y sus hiperparámetros (si aplica)\n",
    "model_configs = {\n",
    "    \"GradientBoosting\": {\n",
    "         \"estimator\": GradientBoostingRegressor(random_state=42),\n",
    "         \"param_grid\": {\n",
    "             \"regressor__n_estimators\": [100, 200],\n",
    "             \"regressor__max_depth\": [3, 5, 7],\n",
    "             \"regressor__learning_rate\": [0.01, 0.1, 0.2]\n",
    "         }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "         \"estimator\": RandomForestRegressor(random_state=42),\n",
    "         \"param_grid\": {\n",
    "             \"regressor__n_estimators\": [100, 200, 300],\n",
    "             \"regressor__max_depth\": [None, 10, 20]\n",
    "         }\n",
    "    },\n",
    "    \"LinearRegression\": {\n",
    "         \"estimator\": LinearRegression(),\n",
    "         \"param_grid\": {}  # Sin hiperparámetros para ajustar\n",
    "    },\n",
    "    \"SVR\": {\n",
    "         \"estimator\": SVR(),\n",
    "         \"param_grid\": {\n",
    "             \"regressor__C\": [0.1, 1, 10],\n",
    "             \"regressor__epsilon\": [0.01, 0.1, 0.2]\n",
    "         }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Recorremos cada modelo para entrenarlo y evaluarlo\n",
    "for model_name, config in model_configs.items():\n",
    "    print(\"Entrenando modelo:\", model_name)\n",
    "    \n",
    "    # Definimos el pipeline para el modelo actual\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", config[\"estimator\"])\n",
    "    ])\n",
    "    \n",
    "    # Si se definen hiperparámetros, usamos GridSearchCV; sino, entrenamos directamente\n",
    "    if config[\"param_grid\"]:\n",
    "        search = GridSearchCV(pipeline, config[\"param_grid\"], cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "        search.fit(X_train, y_train)\n",
    "        best_estimator = search.best_estimator_\n",
    "        best_params = search.best_params_\n",
    "    else:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        best_estimator = pipeline\n",
    "        best_params = \"N/A\"\n",
    "    \n",
    "    # Realizamos predicciones y calculamos las métricas\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"Best Params\": best_params,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2,\n",
    "        \"Explained Variance\": explained_var\n",
    "    }\n",
    "    \n",
    "    print(\"Mejores parámetros:\", best_params)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R²:\", r2)\n",
    "    print(\"Varianza explicada:\", explained_var)\n",
    "    print(\"-\"*40)\n",
    "\n",
    "# Opcional: Convertir resultados a DataFrame para una comparación más ordenada\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Resumen comparativo:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
